---
title: "Japanese & Korean politeness: Perception"
author: "Bodo Winter, Kaori Idemaru"
date: "9/17/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preprocessing

This is new perception data on Japanese combined with the data from Brown et al. (2014). We begin by loading the required packages:

```{r load_packages, message = FALSE}
library(lme4) # for mixed models
library(afex) # for likelihood ratio tests
library(ranger) # for random forests
library(tidyverse) # for data processing
library(stringr) # for data processing
```

Let's load in the Japanese data:

```{r load_data, message = FALSE}
# Load:

jper_jpn <- read_csv('../data/JPNpolitenessPerception.csv')
jper_eng <- read_csv('../data/JPNpolitenessPerception_ENG.csv')

# Stats:

nrow(jper_jpn)
nrow(jper_eng)

ncol(jper_jpn)
ncol(jper_eng)

# Show:

jper_jpn %>% print(n = 2, width = Inf)
```

Add a global trial ID. There are 160 data points per subject. We can utilize the fact that vectors will be recycled to match the length of the longer vector.

```{r add_global_ID}
jper_jpn$GlobalTrial <- 1:160
jper_eng$GlobalTrial <- 1:160
```

Create a group variable:

```{r add_group}
jper_jpn$Group <- 'JPN'
jper_eng$Group <- 'ENG'
```

Remove the SessionTimeUtc column, which resists binding rows. Then bind the two data frames together:

```{r bind_rows}
jper_jpn <- select(jper_jpn, -SessionTimeUtc)
jper_eng <- select(jper_eng, -SessionTimeUtc)

both <- bind_rows(jper_jpn, jper_eng)
```

In both the Japanese version of the experiment and the English version, participants are numbered the same way (numerically). We thus need to create unique subject identifiers to keep them separate.

```{r unique_subs}
both <- mutate(both,
               SubUnique = str_c(Group, '_', Subject))
```

Rename the main dependent variable into something more manageable:

```{r rename_DV}
both <- mutate(both,
               Resp = `Decision.RESP[Trial]`)
```

Check the number of NA responses:

```{r check_NAs}
sum(is.na(both$Resp))

# Proportion:

sum(is.na(both$Resp)) / nrow(both)
```

Let's look at missingness by subject:

```{r missingness_by_sub}
sub_NA <- both %>% group_by(SubUnique) %>% 
  summarize(Missing = sum(is.na(Resp))) %>%
  mutate(Proportion = round((Missing) / 160, 2)) %>% 
  print(n = Inf)
```

The missingness is not completely at random: There's more for the Japanese participants, probably because they were thinking more.

We want to have accurate by-subject estimates. Therefore, we exclude subjects for which we have >10% data loss. We want to assess whether this has any impact on the data later on. Therefore, we don't actually exclude the subjects now, but we'll save their IDs for later exclusion.

```{r data_loss}
# Get IDs of bad subs:

bad_subs <- filter(sub_NA, Proportion > 0.1) %>%
  pull(SubUnique)

# Check:

bad_subs
```

## Reconstruct accuracy values

First, let's exclude NAs:

```{r exclude_NAs}
# Save total row number for comparison:

N_complete <- nrow(both)

# Exclude NAs:

both <- filter(both,
               !is.na(Resp))

# Check loss:

1 - (nrow(both) / N_complete)
```

First, we need to check which file was selected as a response.

```{r create_chosen_file, cache = FALSE}
# Select subset of data frame with only the file names:

fileonly <- as.matrix(select(both, SoundFile, SoundFile2))

# Create matrix for indexing based on what's in the Resp column:

index_M <- cbind(1:nrow(both), both$Resp)

# Index:

both$ChosenFile <- fileonly[index_M]
```

Whether something is accurate or not is hidden in the "ChosenFile" column.

```{r pol_resp}
both$PoliteResponse <- str_split(both$ChosenFile,
                                 pattern = '(_|\\.)',
                                 simplify = TRUE)[, 3]
both <- mutate(both,
               PoliteResponse = ifelse(PoliteResponse == 'cas', 
                                       'casual', 'polite'))
```

Create an accuracy measure and make it into a factor for the mixed model analysis later on:

```{r ACC_create}
# Create accuracy:

both$ACC <- NA
both[both$PoliteResponse == 'casual', ]$ACC <- 0
both[both$PoliteResponse == 'polite', ]$ACC <- 1

# Make into factor:

both <- mutate(both,
            ACC_fac = factor(ACC),
            Resp_fac = factor(Resp))
```

## More preprocessing

Recode the `Running[Trial]` column as repetition:

```{r pair_rep}
both <- rename(both,
               PairRepetition = `Running[Trial]`) %>%
  mutate(PairRepetition = ifelse(PairRepetition == 'SenarioList', 1, 2))
```

Rename and log-transform RT:

```{r RT_create}
both <- rename(both,
               RT = `Decision.RT[Trial]`) %>%
  mutate(LogRT = log10(RT + 1))
```

Create item identifier from file name:

```{r item_ID_create}
both <- mutate(both,
               Item = str_extract(SoundFile, '_[0-9]+_'),
               Item = str_replace_all(Item, '_', ''))
```

Add speaker gender information from external file with the production data:

```{r speaker_gender, message = FALSE}
spk <- read_csv('../data/JPNpoliteness_PerceptionStimuli_AcousticData.csv')
spk <- filter(spk, !duplicated(speaker)) %>%
  select(speaker, gender) %>%
  rename(SpeakerNumber = speaker, SpeakerSex = gender)
both <- left_join(both, spk)
```

Create centered variables for all continuous predictors.

```{r cent_preds}
both <- mutate(both,
               LogRT_c = LogRT - mean(LogRT),
               GlobalTrial_c = GlobalTrial - mean(GlobalTrial),
               Trial_c = Trial - mean(Trial))
```

## Coding predictors

Sum-code predictors that feature in interactions. We will leave the main Group effect (Japanese versus English) as treatment coding because that will be easier interpret. English is the reference level due to alphanumeric ordering.

```{r sum_codes}
both <- mutate(both,
               SpeakerSex = factor(SpeakerSex),
               Sex = factor(Sex))
contrasts(both$SpeakerSex) <- contr.sum(2)
contrasts(both$Sex) <- contr.sum(2)
```


Let's center the repetition pair predictor:

```{r center_pair}
both <- mutate(both,
               PairRepetition_c = PairRepetition - 1.5)
```

## Select subsets with/without high missignness

Now we can create the two separate files, depending on whether we had a lot of loss or not:

```{r rid_bad_subs}
# Save total number of data points up to this point:

N_complete <- nrow(both)

# Exclude the bad subjects:

both_red <- filter(both, !(SubUnique %in% bad_subs))

# Check loss:

1 - (nrow(both_red) / N_complete)
```

## Descriptive stats

Check overall accuracy by language:

```{r ACC_avg}
both %>% group_by(Group) %>% 
  summarize(M = mean(ACC))
both_red %>% group_by(Group) %>% 
  summarize(M = mean(ACC))
```

## Linear mixed effects model analysis

Let's fit the main model with afex:

```{r main_model}
ACC_mdl <- mixed(ACC_fac ~ SpeakerSex * Sex + LogRT_c +
                   Group + 
                   Trial_c + PairRepetition_c + 
                   (1|Subject) +
                   (1|Item) +
                   (1|SpeakerNumber),
                 data = both, family = 'binomial',
                 method = 'LRT')
ACC_mdl
```

Let's interpret the full model. The most important thing is the significance of the intercept.

```{r interpret_mdl}
summary(ACC_mdl$full_model)
```

Just as a sanity check, let's see whether the intercept is significant for the Japanese-only model:

```{r jap_only}
jap_mdl <- glmer(ACC_fac ~ SpeakerSex * Sex +
                   LogRT_c +
                   Trial_c + PairRepetition_c + 
                   (1|Subject) +
                   (1|Item) +
                   (1|SpeakerNumber),
                 data = filter(both, Group == 'JPN'),
                 family = 'binomial')
summary(jap_mdl)
```

Yes, it is!

## Repeat the analysis with exclusions for those that have high missingness

```{r repeat_missing}
ACC_mdl_miss <- mixed(ACC_fac ~ SpeakerSex * Sex +
                        LogRT_c +
                        Group + 
                        Trial_c + PairRepetition_c + 
                        (1|Subject) +
                        (1|Item) +
                        (1|SpeakerNumber),
                      data = both_red, family = 'binomial',
                      method = 'LRT')
ACC_mdl_miss
```

Let's interpret the full model. The most important thing is the significance of the intercept.

```{r interpret_mdl_miss}
summary(ACC_mdl_miss$full_model)
```

There still is a significant effect. Just as a sanity check, let's see whether the intercept is significant for the Japanese-only model that excludes the subjects with high missingness:

```{r jap_only_miss}
jap_mdl_miss <- glmer(ACC_fac ~ SpeakerSex * Sex +
                        LogRT_c +
                        Trial_c + PairRepetition_c + 
                        (1|Subject) +
                        (1|Item) +
                        (1|SpeakerNumber),
                      data = filter(both_red, Group == 'JPN'),
                      family = 'binomial')
summary(jap_mdl_miss)
```

The intercept is still significant, even if we exclude those that have high missingness.


## Comparison to Korean perception data (Brown et al., 2014)

Let's load in the Korean data:

```{r load_kor, message = FALSE}
kper <- read_csv('../data/KRNpolitenessPerception_2014_b.csv')
```

Rename main dependent variable:

```{r kor_rename}
kper <- mutate(kper,
               Resp = Decision.RESP)
```

Create a global trial ID variable:

```{r kor_create_trial_IDs}
kper$GlobalTrial <- 1:160
```

Let's look at missingness by subject:

```{r kor_missingness_by_sub}
sub_NA <- kper %>% group_by(Subject) %>% 
  summarize(Missing = sum(is.na(Resp))) %>%
  mutate(Proportion = round((Missing) / 160, 2)) %>% 
  print(n = Inf)
```

Let's exclude this subject.

```{r kor_excl_high_missingness}
bad_sub <- sub_NA %>% filter(Proportion > 0.1) %>% pull(Subject)
kper <- filter(kper, Subject != bad_sub)
```

## Create accuracy measure for Korean:

First, let's exclude NAs:

```{r kor_exclude_NAs}
# Save total row number for comparison:

N_complete <- nrow(kper)

# Exclude NAs:

kper <- filter(kper,
               !is.na(Resp))

# Check loss:

1 - (nrow(kper) / N_complete)
```

First, we need to check which file was selected as a response.

```{r kor_create_chosen_file, cache = FALSE}
# Select subset of data frame with only the file names:

fileonly <- as.matrix(select(kper, SoundFile, SoundFile2))

# Create matrix for indexing based on what's in the Resp column:

index_M <- cbind(1:nrow(kper), kper$Resp)

# Index:

kper$ChosenFile <- fileonly[index_M]
```

Whether something is accurate or not is hidden in the "ChosenFile" column.

```{r kor_pol_resp}
kper$PoliteResponse <- str_split(kper$ChosenFile,
                                 pattern = '(_|\\.)',
                                 simplify = TRUE)[, 3]
kper <- mutate(kper,
               PoliteResponse = ifelse(PoliteResponse == 'pan', 
                                       'casual', 'polite'))
```

Create an accuracy measure and make it into a factor for the mixed model analysis later on:

```{r kor_ACC_create}
# Create accuracy:

kper$ACC <- NA
kper[kper$PoliteResponse == 'casual', ]$ACC <- 0
kper[kper$PoliteResponse == 'polite', ]$ACC <- 1

# Make into factor:

kper <- mutate(kper,
            ACC_fac = factor(ACC),
            Resp_fac = factor(Resp))
```

Create items from file name:

```{r kor_item_create}
kper <- mutate(kper,
               Item = str_extract(SoundFile, '_[0-9]+_'),
               Item = str_replace_all(Item, '_', ''))
```

Recode that running trial column:

```{r kor_recode_trial}
kper <- rename(kper,
               PairRepetition = `Running[Trial]`) %>%
  mutate(PairRepetition = ifelse(PairRepetition == 'SenarioList',
                                 1, 2))
```

Rename the RT variable and log-transform:

```{r kor_clean_RT}
kper <- rename(kper,
               RT = Decision.RT) %>%
  mutate(LogRT = log10(RT + 1))
```

Center continuous variables:

```{r kor_center_cont}
kper <- mutate(kper,
               LogRT_c = LogRT - mean(LogRT),
               GlobalTrial_c = GlobalTrial - mean(GlobalTrial),
               Trial_c = Trial - mean(Trial))
```

Get speaker gender:

```{r kor_gender, message = FALSE}
kper_speaker <- read_csv('../data/KRNpolitenessProduction_2014.csv')
kper_speaker <- filter(kper_speaker, !duplicated(speaker)) %>%
  select(speaker, gender) %>%
	rename(SpeakerNumber = speaker, SpeakerSex = gender)
kper <- left_join(kper, kper_speaker)
```

## Coding predictors

Sum coding will be done after merging this time. Center pair repetition:

```{r kor_centerpairs}
kper <- mutate(kper,
               PairRepetition_c = PairRepetition - 1.5)
```

## Descriptive statistics Korean

Get average accuracy:

```{r kor_mean}
mean(kper$ACC)
```

## Merging the Japanese and Korean data:

Select subsets that match:

```{r subset}
kper_red <- select(kper,
                   LogRT_c, SpeakerSex, Sex,
                   Trial_c, PairRepetition_c,
                   ACC, Subject, Item, SpeakerNumber)
jpn_only <- filter(both_red, Group == 'JPN')
jpn_only <- select(jpn_only,
                   LogRT_c, SpeakerSex, Sex,
                   Trial_c, PairRepetition_c,
                   ACC, Subject, Item, SpeakerNumber)
```

Now the files are ready to be merged:

```{r merge_jpn_kor}
korjap <- bind_rows(kper_red, jpn_only)
korjap$Language <- c(rep('Korean', nrow(kper_red)),
	rep('Japanese', nrow(jpn_only)))
```

Create unique listener, speaker and item identifier:

```{r unique_IDs}
korjap <- mutate(korjap,
               uniqueSub = str_c(Language, '_', Subject),
               uniqueIt = str_c(Language, '_', Item),
               uniqueSpeaker = str_c(Language, '_', SpeakerNumber))
```

## Create main model:

Sum code the sex predictors:

```{r kor_sumcode_sex}
korjap <- mutate(korjap,
               SpeakerSex = factor(SpeakerSex),
               Sex = factor(Sex))
contrasts(korjap$SpeakerSex) <- contr.sum(2)
contrasts(korjap$Sex) <- contr.sum(2)
```

Mixed models with all the control predictors:

```{r mixed_model_comparison}
korjap_mdl <- mixed(ACC ~ Language + 
                      SpeakerSex * Sex +
                      Trial_c + PairRepetition_c + 
                      (1|uniqueSub) +
                      (1|uniqueIt) +
                      (1|uniqueSpeaker),
                    data = korjap, family = 'binomial',
                    method = 'LRT')
summary(korjap_mdl$full_model)
```

Get odds of language effect:

```{r compute_odds_lang}
exp(fixef(korjap_mdl$full_model)['LanguageKorean'])
```


## Setting up data for random forest analysis in Japanese

Load in z-scored processed data from production analysis:

```{r jprod_load, message = FALSE}
jprod <- read_csv('../data/JPNpolitenessProduction.csv')
jprod %>% print(n = 2, width = Inf)
```

Get clean item identifiers out of the jprod data frame:

```{r clean_ident}
#jprod <- mutate(jprod,
                #item = str_extract(item, '([0-9])+'))
```

There are different numbers of data points per participants. So we need to loop through things to calculate difference scores:

```{r create_unique_IDs}
jprod <- mutate(jprod,
                unique_ID = str_c(task, scenario))
all_prod_speakers <- sort(unique(jprod$speaker))
all_items <- sort(unique(jprod$unique_ID))
```

Create results data frame:

```{r setup_for_loop}
all_res <- rep(NA,
               length(all_prod_speakers) * length(all_items) * 11)
all_res <- matrix(all_res,
                  nrow = length(all_prod_speakers) * length(all_items))
all_res[, 1] <- rep(all_prod_speakers,
                    each = length(all_items))
all_res[, 2] <- rep(all_items,
                    times = length(all_prod_speakers))
one_res <- all_res[all_res[, 1] == "1", ][, 3:11]
```

Fill the results data frame with a loop:

```{r loop}
for (i in seq_along(all_prod_speakers)) {
  this_s <- all_prod_speakers[i]
  this_df <- filter(jprod, speaker == this_s)
	
	this_res_df <- one_res
	for (j in seq_along(all_items)) {
	  this_i <- all_items[j]
		this_item <- filter(this_df, unique_ID == this_i)
		this_item <- arrange(this_item, condition2)
	
		if (nrow(this_item) == 2) {
			myM <- as.matrix(select(this_item,
			                        dur, rate, inmd, f0mnhz,
			                        f0sdhz, jitloc,
			                        shimloc, h1mh2mn, mnHNR))
			myM <- myM[2, ] - myM[1, ]	# polite - informal
			this_res_df[j, ] <- myM
			}
		}
	
	all_res[all_res[, 1] == as.character(this_s), 3:11] <- this_res_df
	}
```

Format results data frame:

```{r format_results}
all_res <- as_tibble(all_res)
mycols <- c('dur', 'rate', 'inmd', 'f0mnhz', 'f0sdhz',
            'jitloc', 'shimloc', 'h1mh2mn', 'mnHNR')
colnames(all_res) <- c('speaker', 'unique_ID', mycols)
all_res[, mycols] <- apply(all_res[, mycols],
                           MARGIN = 2, FUN = as.numeric)
```

Rename columns in all_res data frame for mergability:

```{r rename_jper}
all_res <- rename(all_res,
                  SpeakerNumber = speaker)
```

Create a data frame with only Japanese and no missing:

```{r }
jpn_nomiss <- filter(both_red, Group == 'JPN')
```

Get rid of those speakers that are not in jper:

```{r rid_speakers}
all_res <- filter(all_res,
                  SpeakerNumber %in% jpn_nomiss$SpeakerNumber)
```

Z-score within speaker:

```{r z_score_within}
all_speakers <- unique(jpn_nomiss$SpeakerNumber)
for (i in seq_along(all_speakers)) {
  this_s <- all_speakers[i]
	this_df <- filter(all_res, SpeakerNumber == this_s)
	all_res[all_res$SpeakerNumber == this_s, mycols] <- apply(this_df[, mycols],
		MARGIN = 2, FUN = function(x) unlist(scale(x)))
	}
```

Merge perception and production:

```{r per_prod_merge}
all_res <- filter(all_res,
                  str_detect(unique_ID, 'epr'))
all_res <- mutate(all_res,
                  Item = str_extract(unique_ID, '([0-9])+'),
                  Speaker = as.integer(SpeakerNumber),
                  Speaker_Item = str_c(Speaker, '_', Item))
jpn_nomiss <- mutate(jpn_nomiss,
                     Speaker_Item = str_c(SpeakerNumber, '_', Item))
jall <- bind_cols(jpn_nomiss,
                  all_res[match(jpn_nomiss$Speaker_Item,
                                all_res$Speaker_Item), mycols])
```

## Random forest analysis

Build the random forest:

```{r random_forest}
set.seed(42)
jap.forest <- ranger(as.factor(ACC) ~ dur + rate + inmd +
                       f0mnhz + f0sdhz +
                       jitloc + shimloc + h1mh2mn + mnHNR,
                     data = jall,
                     importance = 'permutation',
                     num.trees = 5000, mtry = 3)
```

Explore the variable importances:
  
```{r explore_varimp}
sort(jap.forest$variable.importance)
```

Get predictions to double check classification accuracy:

```{r get_preds_ACC}
jap.forest_preds <- predict(jap.forest, data = jall)
mypreds <- table(jall$ACC,
                 jap.forest_preds$predictions)
round(sum(diag(mypreds)) / sum(mypreds), 2)	# 63%

# Compare to how many ACC's there are overall to look at baseline:

mean(jall$ACC)
```

## More conservative random forest analysis

Let's split dataset into test and training set. Let's go for a 70%-30% split. How big should the test and training sets be in this case?

```{r test_train_set_size}
nrow(jall)
train_N <- ceiling(0.7 * nrow(jall))
train_N
test_N <- nrow(jall) - train_N
```

Split the data into test and training:

```{r test_train}
set.seed(42)
train_ids <- sample(1:nrow(jall), train_N)
test_ids <- (1:nrow(jall))[!(1:nrow(jall) %in% train_ids)]
jall_train <- jall[train_ids, ]
jall_test <- jall[test_ids, ]
```

Train random forest on training set:

```{r random_forest_train}
set.seed(42)
jap.forest_train <- ranger(as.factor(ACC) ~ dur + rate +
                             inmd + f0mnhz + f0sdhz +
                             jitloc + shimloc + h1mh2mn +
                             mnHNR, data = jall_train,
                           importance = 'permutation',
                           num.trees = 5000, mtry = 3)
```

Get predictions for test data:

```{r random_forest_validate}
jap_forest_crossv <- predict(jap.forest_train, data = jall_test)
mypreds_test <- table(jall_test$ACC,
                      jap_forest_crossv$predictions)
round(sum(diag(mypreds_test)) / sum(mypreds_test), 2)	# around 0.61
```

Compare variable importances:

```{r compare_varimps}
sort(jap.forest_train$variable.importance)
sort(jap.forest$variable.importance)
```

Duration is consistently on top. Intensity, shimmer and rate as well.

## Plotting variable importance

Let's make a plot of this. First, getting nice variable names for the plot:

```{r}
myvars <- sort(jap.forest$variable.importance)
names(myvars)[names(myvars) == 'mnHNR'] <- 'HNR'
names(myvars)[names(myvars) == 'f0mnhz'] <- 'Pitch'
names(myvars)[names(myvars) == 'h1mh2mn'] <- 'H1-H2'
names(myvars)[names(myvars) == 'jitloc'] <- 'Jitter'
names(myvars)[names(myvars) == 'f0sdhz'] <- 'Pitch SD'
names(myvars)[names(myvars) == 'rate'] <- 'Rate'
names(myvars)[names(myvars) == 'shimloc'] <- 'Shimmer'
names(myvars)[names(myvars) == 'inmd'] <- 'Intensity'
names(myvars)[names(myvars) == 'dur'] <- 'Duration'
myvars
```


## Plot of variable importances:

```{r plot_varimps, fig.width = 10, fig.height = 6}
quartz('', 9, 6)
par(mai = c(1.5, 1.5, 0.5, 0.5))
plot(1, 1, type = 'n', xlab = '', ylab = '',
     xaxt = 'n', yaxt = 'n', bty = 'n',
     xlim = c(0, 0.03), ylim = c(0, 10))
mtext(side = 1, text = 'Relative Variable Importance',
      line = 4, font = 2, cex = 2)
mtext(side = 1, text = '(permutation based)',
      line = 6, font = 2, cex = 1.5)
abline(h = 1:9, lty = 2, col = 'darkgrey')
axis(side = 1, at = seq(0, 0.03, 0.01),
     lwd = 2, cex.axis = 1.5, font = 2)
axis(side = 2, at = 1:9, labels = names(myvars),
     lwd = 2, cex.axis = 1.5, font = 2, las = 2)
points(x = myvars, y = 1:9, pch = 15, cex = 1.5)
box(lwd = 2)
abline(v = 0)
quartz.save('../figures/random_forest_variable_importances.png',
            dpi = 300)
quartz.save('../figures/random_forest_variable_importances.tiff',
            dpi = 300)
```

This completes this analysis.
